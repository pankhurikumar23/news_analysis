{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from nltk.sentiment.util import *\n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_list = [\"EconomicTimes\", \"Hindu\", \"HindustanTimes\", \"IndianExpress\", \"TimesOfIndia\"]\n",
    "REPLACE_NO_SPACE = re.compile(\"(\\')|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])\")\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\/)|(<p>)|(\\\\n)|(\\;)|(\\:)\")\n",
    "REPLACE_WITH_PERIOD = re.compile(\"(\\-)|(\\!)|(\\?)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "articlesLST = []\n",
    "issues = ['Ayodhya', 'TripleTalaq']\n",
    "def conductAnalysis():\n",
    "    for issue in issues:\n",
    "        print(issue)\n",
    "        for source in source_list:\n",
    "            filename = \"data/\" + issue + '/' + source\n",
    "            file = open(filename, 'rb')\n",
    "            articles = pickle.load(file)\n",
    "            print(source + \" \" + str(len(articles)))\n",
    "\n",
    "            articles = [REPLACE_NO_SPACE.sub(\"\", article) for article in articles]\n",
    "            articles = [REPLACE_WITH_SPACE.sub(\" \", article) for article in articles]\n",
    "            articles = [REPLACE_WITH_PERIOD.sub(\".\", article) for article in articles]\n",
    "\n",
    "            analysis = []\n",
    "            count = [0, 0]\n",
    "            for article in articles:\n",
    "                thing = TextBlob(article)\n",
    "                if thing.sentiment[0] > 0.1:\n",
    "                    count[0] += 1\n",
    "                else:\n",
    "                    count[1] += 1\n",
    "                analysis.append({'polarity': thing.sentiment[0], 'subjectivity': thing.sentiment[1]})\n",
    "                articlesLST.append([source, article, thing.sentiment[0], thing.sentiment[1]])\n",
    "    #         print(count)\n",
    "\n",
    "            filename1 = filename + '_clean'\n",
    "            file = open(filename1, 'wb')\n",
    "            pickle.dump(articles, file)\n",
    "            file.close()\n",
    "            filename2 = filename + '_scores'\n",
    "            file = open(filename2, 'wb')\n",
    "            pickle.dump(analysis, file)\n",
    "        articlesDF = pd.DataFrame(articlesLST,columns=['source', 'article', 'polarity', 'subjectivity'])\n",
    "        file1 = issue + 'Articles Dataframe'\n",
    "        file = open(file1, 'wb')\n",
    "        pickle.dump(articlesDF, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ayodhya\n",
      "EconomicTimes 69\n",
      "Hindu 124\n",
      "HindustanTimes 83\n",
      "IndianExpress 121\n",
      "TimesOfIndia 208\n",
      "TripleTalaq\n",
      "EconomicTimes 88\n",
      "Hindu 242\n",
      "HindustanTimes 339\n",
      "IndianExpress 366\n",
      "TimesOfIndia 693\n"
     ]
    }
   ],
   "source": [
    "# uses textblob to generate sentiments\n",
    "conductAnalysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from pickle\n",
    "file = open(\"TripleTalaqArticles Dataframe\", 'rb')\n",
    "articlesDF_tt = pickle.load(file)\n",
    "file = open(\"AyodhyaArticles Dataframe\", 'rb')\n",
    "articlesDF_ay = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize sentences into POS\n",
    "# find sentences that contain certain words related to BJP, or Ayodhya/talaq\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sentList = []\n",
    "bjp = [\"Narendra Modi\", \"Prime Minister\", \"BJP\", \"Bharatiya Janata Party\"]\n",
    "ayodhya = [\"Ayodhya\", \"mandir\", \"Supreme Court\", \"Uttar Pradesh\", \"U.P.\", 'Ramjanmabhoomi', \n",
    "           'Babri', 'Masjid', 'masjid', 'akhara', 'hindu', 'waqf', 'Hindu', 'Waqf', 'Akhara']\n",
    "talaq = ['triple', 'Triple', 'Talaq', 'talaq', 'Shayara', 'Bano', 'Muslim Personal', 'AIMPLB', 'instant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in articlesDF_ay.iterrows():\n",
    "    sentences = row['article'].split('.')\n",
    "    for sentence in sentences:\n",
    "        text = word_tokenize(sentence)\n",
    "        if (not set(bjp).isdisjoint(text)) or (not set(ayodhya).isdisjoint(text)):\n",
    "            sentMap = {'source': row['source'], 'sentence': sentence, 'score': ''}\n",
    "            sentList.append(sentMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21875\n"
     ]
    }
   ],
   "source": [
    "sentencesDF = pd.DataFrame(sentList,columns=['source', 'sentence', 'score'])\n",
    "print(len(sentencesDF))\n",
    "\n",
    "file = open('Ayodhya_Sentences', 'wb')\n",
    "pickle.dump(sentencesDF, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in articlesDF_tt.iterrows():\n",
    "    sentences = row['article'].split('.')\n",
    "    for sentence in sentences:\n",
    "        text = word_tokenize(sentence)\n",
    "        if (not set(bjp).isdisjoint(text)) or (not set(talaq).isdisjoint(text)):\n",
    "            sentMap = {'source': row['source'], 'sentence': sentence, 'score': ''}\n",
    "            sentList.append(sentMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34128\n"
     ]
    }
   ],
   "source": [
    "sentencesDF = pd.DataFrame(sentList,columns=['source', 'sentence', 'score'])\n",
    "print(len(sentencesDF))\n",
    "\n",
    "file = open('TripleTalaq_Sentences', 'wb')\n",
    "pickle.dump(sentencesDF, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from file\n",
    "file = open('TripleTalaq_Sentences', 'rb')\n",
    "sentencesDF_tt = pickle.load(file)\n",
    "\n",
    "file = open('Ayodhya_Sentences', 'rb')\n",
    "sentencesDF_ay = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store random 20% of each publication's sentences into a CSV\n",
    "pubCount = sentencesDF_tt['source'].value_counts()\n",
    "train_tt, test_tt = train_test_split(sentencesDF_tt, test_size = 0.8, stratify=sentencesDF_tt.source)\n",
    "\n",
    "pubCount = sentencesDF_ay['source'].value_counts()\n",
    "train_ay, test_ay = train_test_split(sentencesDF_ay, test_size = 0.8, stratify=sentencesDF_ay.source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('TripleTalaq_Train', 'wb')\n",
    "pickle.dump(train_tt, file)\n",
    "train_tt.to_csv('TripleTalaq_train.csv')\n",
    "\n",
    "file = open('TripleTalaq_Test', 'wb')\n",
    "pickle.dump(test_tt, file, protocol=2)\n",
    "\n",
    "file = open('Ayodhya_Train', 'wb')\n",
    "pickle.dump(train_ay, file)\n",
    "train_ay.to_csv('TripleTalaq_train.csv')\n",
    "\n",
    "file = open('Ayodhya_Test', 'wb')\n",
    "pickle.dump(test_ay, file, protocol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start From Here when running analysis\n",
    "Everything above this is pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6dcf04167e41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# read from file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_ay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Ayodhya_train_annotated.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Ayodhya_Test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_ay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# read from file\n",
    "train_ay = pd.read_csv('Ayodhya_train_annotated.csv')\n",
    "file = open('Ayodhya_Test', 'rb')\n",
    "test_ay = pickle.load(file)\n",
    "\n",
    "train_tt = pd.read_csv('Ayodhya_train_annotated.csv')\n",
    "file = open('TripleTalaq_Sentences', 'rb')\n",
    "test_tt = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    " \n",
    "sentiment_data = zip(train[\"sentence\"], train[\"score\"]) \n",
    "train_X, train_y = zip(*sentiment_data)\n",
    "\n",
    "test_data = zip(test['sentence'], test['score'])\n",
    "test_X, test_y = zip(*test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.sentiment.util import mark_negation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.base import TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_bigram_clf = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(analyzer=\"word\",\n",
    "                                   ngram_range=(1, 2),\n",
    "                                   tokenizer=word_tokenize,\n",
    "                                   # tokenizer=lambda text: mark_negation(word_tokenize(text)),\n",
    "                                   preprocessor=lambda text: text.replace(\"<br />\", \" \"),)),\n",
    "    ('classifier', LinearSVC())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2),\n",
       "        preprocessor=<function <lambda> at ...ax_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0))])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_bigram_clf.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = unigram_bigram_clf.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['score'] = results.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('TripleTalaq_test_results', 'wb')\n",
    "pickle.dump(test, file)\n",
    "test.to_csv('TripleTalaqtest_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from file\n",
    "file = open('TripleTalaq_test_results', 'rb')\n",
    "test_res = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>sentence</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EconomicTimes</td>\n",
       "      <td>Politics and Nation Triple</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EconomicTimes</td>\n",
       "      <td>talaq supporters collect ‘5 lakh signatures’  ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EconomicTimes</td>\n",
       "      <td>Making this claim Jalsa Tehaffuz Personal Law ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EconomicTimes</td>\n",
       "      <td>“Article 25 of the Constitution gives equal op...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EconomicTimes</td>\n",
       "      <td>The unilateral triple talaq which Muslims a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          source                                           sentence  score\n",
       "0  EconomicTimes                         Politics and Nation Triple      0\n",
       "1  EconomicTimes  talaq supporters collect ‘5 lakh signatures’  ...      0\n",
       "2  EconomicTimes  Making this claim Jalsa Tehaffuz Personal Law ...      0\n",
       "3  EconomicTimes  “Article 25 of the Constitution gives equal op...     -1\n",
       "4  EconomicTimes     The unilateral triple talaq which Muslims a...      0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source          score\n",
       "EconomicTimes   -1         58\n",
       "                 0        476\n",
       "                 1         35\n",
       "Hindu           -1        157\n",
       "                 0        946\n",
       "                 1         85\n",
       "HindustanTimes  -1        138\n",
       "                 0       1720\n",
       "                 1        130\n",
       "IndianExpress   -1        205\n",
       "                 0       1707\n",
       "                 1        143\n",
       "TimesOfIndia    -1        341\n",
       "                 0       2766\n",
       "                 1        255\n",
       "dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_res.groupby(['source', 'score']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EconomicTimes</th>\n",
       "      <td>-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hindu</th>\n",
       "      <td>-72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HindustanTimes</th>\n",
       "      <td>-8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IndianExpress</th>\n",
       "      <td>-62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TimesOfIndia</th>\n",
       "      <td>-86</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                score\n",
       "source               \n",
       "EconomicTimes     -23\n",
       "Hindu             -72\n",
       "HindustanTimes     -8\n",
       "IndianExpress     -62\n",
       "TimesOfIndia      -86"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_res.groupby(['source']).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy: 0.633"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
